{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -e ../python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from explainprompt.logger import Logger, MemoryLogHandler\n",
    "from explainprompt.model import (\n",
    "    Prompt, \n",
    "    Section, \n",
    "    ModelResponse)\n",
    "\n",
    "from explainprompt.jupyter import ExplainPromptWidget\n",
    "\n",
    "from langchain import PromptTemplate, OpenAI, LLMChain\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.callbacks.tracers.base import BaseTracer\n",
    "from langchain.callbacks.tracers.schemas import (\n",
    "    Run,\n",
    "    TracerSession,\n",
    ")\n",
    "from typing import Any, Dict, Optional, Union\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Union\n",
    "from uuid import UUID\n",
    "\n",
    "from langchain.callbacks.tracers.base import BaseTracer\n",
    "from langchain.callbacks.tracers.schemas import Run, TracerSession\n",
    "from langchain.load.dump import dumpd\n",
    "from langchain.schema.messages import BaseMessage\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplainPromptTracer(BaseTracer):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        example_id: Optional[Union[UUID, str]] = None,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the LangChain tracer.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.session: Optional[TracerSession] = None\n",
    "        self.example_id = (\n",
    "            UUID(example_id) if isinstance(example_id, str) else example_id\n",
    "        )\n",
    "        self.tags = tags or []\n",
    "        self.logger = MemoryLogHandler()\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        messages: List[List[BaseMessage]],\n",
    "        *,\n",
    "        run_id: UUID,\n",
    "        tags: Optional[List[str]] = None,\n",
    "        parent_run_id: Optional[UUID] = None,\n",
    "        metadata: Optional[Dict[str, Any]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Start a trace for an LLM run.\"\"\"\n",
    "        parent_run_id_ = str(parent_run_id) if parent_run_id else None\n",
    "        execution_order = self._get_execution_order(parent_run_id_)\n",
    "        start_time = datetime.utcnow()\n",
    "        if metadata:\n",
    "            kwargs.update({\"metadata\": metadata})\n",
    "        chat_model_run = Run(\n",
    "            id=run_id,\n",
    "            parent_run_id=parent_run_id,\n",
    "            serialized=serialized,\n",
    "            inputs={\"messages\": [[dumpd(msg) for msg in batch] for batch in messages]},\n",
    "            extra=kwargs,\n",
    "            events=[{\"name\": \"start\", \"time\": start_time}],\n",
    "            start_time=start_time,\n",
    "            execution_order=execution_order,\n",
    "            child_execution_order=execution_order,\n",
    "            run_type=\"llm\",\n",
    "            tags=tags,\n",
    "        )\n",
    "        self._start_trace(chat_model_run)\n",
    "        self._on_chat_model_start(chat_model_run)\n",
    "\n",
    "    def _persist_run(self, run: Run) -> None:\n",
    "        \"\"\"The Langchain Tracer uses Post/Patch rather than persist.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_llm_start(self, run: Run) -> None:\n",
    "        \"\"\"Persist an LLM run.\"\"\"\n",
    "        for prompt in run.inputs['prompts']:\n",
    "            self.logger.log_prompt(Prompt(label=\"Prompt\", sections=[\n",
    "                Section(content=prompt)\n",
    "            ]))\n",
    "\n",
    "    def _on_chat_model_start(self, run: Run) -> None:\n",
    "        \"\"\"Persist an LLM run.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_llm_end(self, run: Run) -> None:\n",
    "        \"\"\"Process the LLM Run.\"\"\"\n",
    "        for generation in run.outputs['generations']:\n",
    "            self.logger.log_response(ModelResponse(sections=[\n",
    "                Section(content=generation[0]['text'])\n",
    "            ]))\n",
    "\n",
    "    def _on_llm_error(self, run: Run) -> None:\n",
    "        \"\"\"Process the LLM Run upon error.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_chain_start(self, run: Run) -> None:\n",
    "        \"\"\"Process the Chain Run upon start.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_chain_end(self, run: Run) -> None:\n",
    "        \"\"\"Process the Chain Run.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_chain_error(self, run: Run) -> None:\n",
    "        \"\"\"Process the Chain Run upon error.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_tool_start(self, run: Run) -> None:\n",
    "        \"\"\"Process the Tool Run upon start.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_tool_end(self, run: Run) -> None:\n",
    "        \"\"\"Process the Tool Run.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_tool_error(self, run: Run) -> None:\n",
    "        \"\"\"Process the Tool Run upon error.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_retriever_start(self, run: Run) -> None:\n",
    "        \"\"\"Process the Retriever Run upon start.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_retriever_end(self, run: Run) -> None:\n",
    "        \"\"\"Process the Retriever Run.\"\"\"\n",
    "        return\n",
    "\n",
    "    def _on_retriever_error(self, run: Run) -> None:\n",
    "        \"\"\"Process the Retriever Run upon error.\"\"\"\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"prompt\",\n",
      "    \"label\": \"Prompt\",\n",
      "    \"sections\": [\n",
      "      {\n",
      "        \"content\": \"What is a good name for a company that makes colorful socks?\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"response\",\n",
      "    \"sections\": [\n",
      "      {\n",
      "        \"content\": \"\\n\\nSocktastic!\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "tracer = ExplainPromptTracer()\n",
    "\n",
    "prompt_template = \"What is a good name for a company that makes {product}?\"\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PromptTemplate.from_template(prompt_template)\n",
    ")\n",
    "\n",
    "llm_chain.run(\n",
    "    \"colorful socks\",\n",
    "    callbacks=[tracer]\n",
    ")\n",
    "\n",
    "messages = list(map(lambda x: x.model_dump(exclude_none=True), tracer.logger._messages))\n",
    "\n",
    "print(json.dumps(messages, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcf8182878045b09c81e7a1e8da71c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExplainPromptWidget(data='{\"trajectory\": [{\"type\": \"response\", \"sections\": [{\"content\": \"colorful socks\"}]}, {â€¦"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "widget = ExplainPromptWidget()\n",
    "widget.data = json.dumps({\"trajectory\":messages})\n",
    "widget.theme = 'dark'\n",
    "widget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
