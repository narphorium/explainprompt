<html>
<head>
    <title>ExplainPrompt &mdash; Large Language Models Are Zero-Shot Reasoners</title>
    <link rel="icon" href="../logo.svg" type="image/svg"/>
    <link rel="preconnect" href="https://rsms.me/">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    <link rel="stylesheet" href="https://rsms.me/inter/inter.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@100;200;300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/vscode-codicons@0.0.17/dist/codicon.min.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/common.css">
    <link rel="stylesheet" href="../css/walkthrough.css">
    
    <script src="https://unpkg.com/hotkeys-js/dist/hotkeys.min.js"></script>
    <script src="../js/walkthrough.js"></script>
</head>
<body>
<div class="aside">
    <div class="breadcrumbs">
        <a href="../index.html">ExplainPrompt</a>
        <span class="material-icons material-symbols-outlined">chevron_right</span>
        Papers
    </div>
    <h1>Large Language Models Are Zero-Shot Reasoners</h1>
    <div class="citation">
        <div class="authors">Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa</div>
        <a target="_blank" href="http://arxiv.org/abs/2205.11916">arXiv:2205.11916</a> (<span class="year">2022</span>)
        <a target="_blank" href="https://github.com/kojima-takeshi188/zero_shot_cot"><i class="codicon codicon-github-inverted"></i></a>
    </div>
    <div id="controls"></div>
    <div class="description" data-step="0"><em>Summary:</em> This research shows that large language models, like AI chatbots, can be really good at reasoning tasks even without specific training. By simply using the prompt "Let's think step by step" before each answer, the models show remarkable improvement in their ability to solve complex problems. The researchers call this approach "Zero-Shot Chain-of-Thought". It performed significantly better on a variety of tasks, like math and logical reasoning, without needing few-shot examples to learn from. This discovery could save time and resources, and it also suggests that these models might have more untapped potential than we thought.</div>
    <div class="description" data-step="1">This is a sample question from the <a targe="_blank" href="https://gitlab.cs.washington.edu/ALGES/TACL2015">SingleEQ dataset</a> which is used to evaluate Chain of Thought prompting on algebraic word problems.</div>
    <div class="description" data-step="2">The prompt starts with the original input question. There are no examples question/answer pairs like in few-shot <a href="chain-of-thought.html">Chain-of-Thought</a> prompting.</div>
    <div class="description" data-step="3">
        <p>In few-shot <a href="chain-of-thought.html">Chain-of-Thought</a> prompting this is just "A:". But in "Large Language Models Are Zero-Shot Reasoners" the authors found that they can induce the same behaviour without the few-shot examples simply by starting the answer with "Let's think step by step.". This is what makes it a zero-shot prompt.</p>
        <p>Here's the full list of answer prefixes that they tried:</p>
        <table class="results">
            <thead>
                <tr><th>Prefix</th><th class="numerical">Accuracy</th></tr>
            </thead>
            <tbody>
                <tr><td>"Let's think step by step."</td><td class="numerical">78.7</td></tr>
                <tr><td>"First, "</td><td class="numerical">77.3</td></tr>
                <tr><td>"Let’s think about this logically."</td><td class="numerical">74.5</td></tr>
                <tr><td>"Let’s solve this problem by splitting it into steps."</td><td class="numerical">72.2</td></tr>
                <tr><td>"Let’s be realistic and think step by step."</td><td class="numerical">70.8</td></tr>
                <tr><td>"Let’s think like a detective step by step."</td><td class="numerical">70.3</td></tr>
                <tr><td>"Let’s think"</td><td class="numerical">57.5</td></tr>
                <tr><td>"Before we dive into the answer,"</td><td class="numerical">55.7</td></tr>
                <tr><td>"The answer is after the proof."</td><td class="numerical">45.7</td></tr>
                <tr><td>No prefix</td><td class="numerical">17.7</td></tr>
            </tbody>
        </table>
    </div>
    <div class="description" data-step="4">Now, the model is able to generate a step-by-step reasoning for its answer without having been finetuned or shown any examples of Chain-of-Thought prompts.</div>
    <div class="description" data-step="5">In order to cleanly extract the model's final answer, the authors add a 2nd prompt with a "trigger sentence". </div>
    <div class="description" data-step="6">The final answer is short and doesn't contain any of the reasoning steps. This makes it easier to match against the known answer and calculate the overall accuracy of the prompts on an evaluation dataset.</div>
</div>
<script>
    new Walkthrough(document.getElementById('controls'));
</script>
<div class="trajectory">
    <div class="conversation">
        <div class="response" data-step="1">
            <div class="question"><em>Q:</em> A spaceship traveled 0.5 of a light-year from Earth to Planet X and 0.1 of a light-year from Planet X to Planet Y. Then it traveled 0.1 of a light-year from Planet Y back to Earth. How many light-years did the spaceship travel in all?</div>
        </div>
        <details class="prompt">
            <summary>Chain-of-Thought Prompt</summary>
            <div>
                <mark data-step="2"><em>Q:</em> A spaceship traveled 0.5 of a light-year from Earth to Planet X and 0.1 of a light-year from Planet X to Planet Y. Then it traveled 0.1 of a light-year from Planet Y back to Earth. How many light-years did the spaceship travel in all?</mark><br/>
                <em>A:</em> <mark data-step="3">Let’s think step by step.</mark>
            </div>
        </details>
        <div class="response">
            <div class="answer"><em>A:</em> Let’s think step by step. <mark data-step="4">The spaceship traveled 0.5 light-years from Earth to Planet X. Then it traveled 0.1 light-years from Planet X to Planet Y. Finally, it traveled 0.1 light-years from Planet Y back to Earth. So the total distance the spaceship traveled is 0.5 + 0.1 + 0.1 = 0.7 light-years.</mark></div>
        </div>
        <details class="prompt">
            <summary>Answer Prompt</summary>
            <details class="scratchpad">
                <summary>Scratchpad</summary>
                <div>
                    <em>Q:</em> A spaceship traveled 0.5 of a light-year from Earth to Planet X and 0.1 of a light-year from Planet X to Planet Y. Then it traveled 0.1 of a light-year from Planet Y back to Earth. How many light-years did the spaceship travel in all?<br/>
                <em>A:</em> Let’s think step by step. The spaceship traveled 0.5 light-years from Earth to Planet X. Then it traveled 0.1 light-years from Planet X to Planet Y. Finally, it traveled 0.1 light-years from Planet Y back to Earth. So the total distance the spaceship traveled is 0.5 + 0.1 + 0.1 = 0.7 light-years.<br/>
                </div>
            </details>
            <div><mark data-step="5">Therefore, the answer (arabic numerals) is:</mark></div>
        </details>
        <div class="response" data-step="6">
            <div class="answer">0.7 light-years</div>
        </div>
    </div>
</div>
</body>
</html>


